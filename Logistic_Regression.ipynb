{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxqgrEp6CEM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Assignment Theory"
      ],
      "metadata": {
        "id": "v6EC3x-56HpV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hLJNQ4Sr6IVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "-Logistic Regression and Linear Regression are both supervised learning algorithms used in machine learning and statistics, but they are used for different types of problems and have different underlying models. Here's a breakdown: Used for predicting continuous outcomes.\n",
        "\n",
        "Example: Predicting house prices, temperature, or stock prices.\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "Used for predicting categorical outcomes, typically binary classification (e.>g., 0 or 1, yes or no, spam or not spam).\n",
        "\n",
        "Can be extended to multiclass classification with techniques like One-vs-Rest.\n",
        "\n",
        "Output Linear Regression: Predicts a real-valued number.\n",
        "\n",
        "Output is unbounded (can be any value from âˆ’ âˆ âˆ’âˆ to + âˆ +âˆ).\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "Predicts a probability that the input belongs to a certain class.\n",
        "\n",
        "Output is bounded between 0 and 1 (via the sigmoid function).\n",
        "\n",
        "Mathematical Form Linear Regression: ğ‘¦ = ğ›½ 0 + ğ›½ 1 ğ‘¥ 1 + ğ›½ 2 ğ‘¥ 2 + â‹¯ + ğ›½ ğ‘› ğ‘¥ ğ‘› y=Î² 0â€‹+Î² 1â€‹x 1â€‹+Î² 2â€‹x 2â€‹+â‹¯>+Î² nâ€‹x nâ€‹\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "ğ‘ = 1 1 + ğ‘’ âˆ’ ( ğ›½ 0 + ğ›½ 1 ğ‘¥ 1 + ğ›½ 2 ğ‘¥ 2 + â‹¯ + ğ›½ ğ‘› ğ‘¥ ğ‘› ) p= 1+e âˆ’(Î² >0â€‹+Î² 1â€‹x 1â€‹+Î² 2â€‹x 2â€‹+â‹¯+Î² nâ€‹x nâ€‹)\n",
        "\n",
        "1â€‹\n",
        "\n",
        "Where ğ‘ p is the probability that the output is class 1.\n",
        "\n",
        "Loss Function Linear Regression: Uses Mean Squared Error (MSE). Logistic Regression: Uses Log Loss (Cross-Entropy Loss).\n",
        "\n",
        "Interpretation of Coefficients Linear Regression: Coefficients represent the change in the output for a one-unit change in the input. Logistic Regression: Coefficients represent the log-odds of the outcome for a one-unit change in the input.\n",
        "\n",
        "Application Examples Linear Regression: Predicting salary based on years of experience.\n",
        "\n",
        "Estimating sales based on advertising budget.\n",
        "\n",
        "Logistic Regression:\n",
        "\n"
      ],
      "metadata": {
        "id": "KtfSPSIg6JA_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsdIwvYm6SNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the mathematical equation of Logistic Regression?\n",
        "The mathematical equation of Logistic Regression is based on the logistic (sigmoid) function, which models the probability that a given input belongs to a certain class (usually binary classification: 0 or 1).\n",
        "\n",
        "Logistic (Sigmoid) Function The core of logistic regression is the sigmoid function: ğœ ( ğ‘§ ) = 1 1 + ğ‘’ âˆ’ ğ‘§ Ïƒ(z)= 1+e âˆ’z\n",
        "\n",
        "This function maps any real-valued number to the range ( 0 , 1 ) (0,1), which can be interpreted as a probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "MUHRvY-R6Sj1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZWgB5YI6YMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3..Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "-We use the sigmoid function in logistic regression because it transforms the linear output of a model into a probability, which is essential for binary classification tasks.\n",
        "\n",
        "Here's a breakdown of why it's used:\n",
        "\n",
        "Converts Any Real Value to a Probability The sigmoid function is defined as: ğœ ( ğ‘§ ) = 1 1 + ğ‘’ âˆ’ ğ‘§ Ïƒ(z)= 1+e âˆ’z\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘§ z is the linear combination of input features and weights (i.e., ğ‘§ = ğ‘¤ ğ‘‡ ğ‘¥ + ğ‘ z=w T x+b).\n",
        "\n",
        "This function maps any real number to a value between 0 and 1, which can be interpreted as the probability of the positive class (e.g., probability of \"yes\", or \"class 1\")."
      ],
      "metadata": {
        "id": "ubQ6zrHY6YvH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wChwPTmw6flb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the cost function of Logistic Regression?\n",
        "\n",
        "-The cost function of Logistic Regression is based on the log loss, also known as the binary cross-entropy loss. It measures how well the predicted probabilities match the actual class labels (0 or 1).\n",
        "\n",
        "Logistic Regression Cost Function Given:\n",
        "\n",
        "ğ‘š m: number of training examples\n",
        "\n",
        "ğ‘¦ ( ğ‘– ) y (i) : actual label (0 or 1) for the ğ‘– i-th example\n",
        "\n",
        "ğ‘¦ ^ ( ğ‘– ) = â„ ğœƒ ( ğ‘¥ ( ğ‘– ) ) = 1 1 + ğ‘’ âˆ’ ğœƒ ğ‘‡ ğ‘¥ ( ğ‘– ) y ^â€‹\n",
        "\n",
        "(i) =h Î¸â€‹(x (i) )= 1+e âˆ’Î¸ T x (i)\n",
        "\n",
        "1â€‹: predicted probability (sigmoid output)\n",
        "\n",
        "The cost function ğ½ ( ğœƒ ) J(Î¸) is:\n",
        "\n",
        "ğ½ ( ğœƒ ) = âˆ’ 1 ğ‘š âˆ‘ ğ‘– = 1 ğ‘š [ ğ‘¦ ( ğ‘– ) log â¡ ( ğ‘¦ ^ ( ğ‘– ) ) + ( 1 âˆ’ ğ‘¦ ( >ğ‘– ) ) log â¡ ( 1 âˆ’ ğ‘¦ ^ ( ğ‘– ) ) ] J(Î¸)=âˆ’ m 1â€‹\n",
        "\n",
        "i=1 âˆ‘ mâ€‹[y (i) log( y ^â€‹\n",
        "\n",
        "(i) )+(1âˆ’y (i) )log(1âˆ’ y ^â€‹\n",
        "\n",
        "(i) )"
      ],
      "metadata": {
        "id": "AaXDpOCc6gA5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKH0WdOP6nNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "-Regularization in logistic regression is a technique used to prevent overfitting by penalizing large coefficients in the model. It modifies the cost function to include an additional term that discourages the model from fitting the training data too closely, which can hurt its performance on unseen dat\n",
        "\n",
        "Why Is it need:In logistic regression (or any model), overfitting happens when:\n",
        "\n",
        "The model becomes too complex (e.g., too many features or too large coefficients).\n",
        "\n",
        "It captures noise instead of the underlying pattern.\n",
        "\n",
        "It performs well on training data but poorly on new/unseen data.\n",
        "\n",
        "Regularization helps by constraining the optimization process:\n",
        "\n",
        "It keeps the learned weights (coefficients) small.\n",
        "\n",
        "This encourages the model to focus on the most important patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "PLfBOsX96nin"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wXsL3kmu6u0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "-Lasso, Ridge, and Elastic Net are regularized regression techniques used to prevent overfitting and improve model generalization, especially when dealing with high-dimensional data (many features). Here's how they differ: Ridge Regression (L2 Regularization) Penalty Term: Adds the sum of squared coefficients (L2 norm) to the loss function. Loss = RSS + ğœ† âˆ‘ ğ‘— = 1 ğ‘ ğ›½ ğ‘— 2 Loss=RSS+Î» j=1 âˆ‘ pâ€‹Î² j 2â€‹\n",
        "\n",
        "Effect: Shrinks coefficients towards zero but does not set them exactly to zero.\n",
        "\n",
        "Use Case: Best when all features are relevant and multicollinearity exists.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Works well when many small/medium-sized effects are present.\n",
        "\n",
        "Stabilizes coefficients in the presence of multicollinearity.\n",
        "\n",
        "Lasso Regression (L1 Regularization) Penalty Term: Adds the sum of the absolute values of the coefficients (L1 norm). Loss = RSS + ğœ† âˆ‘ ğ‘— = 1 ğ‘ âˆ£ ğ›½ ğ‘— âˆ£ Loss=RSS+Î» j=1 âˆ‘ pâ€‹âˆ£Î² jâ€‹âˆ£ Effect: Can shrink some coefficients exactly to zero, thus performing feature selection.\n",
        "\n",
        "Use Case: Best when you expect only a few features to be important.\n",
        "\n",
        "Performs variable selection.\n",
        "\n",
        "Produces sparse models, which are easier to interpret.\n",
        "\n",
        "Elastic Net Regression Penalty Term: A combination of L1 and L2 regularization. Loss = RSS + ğœ† 1 âˆ‘ ğ‘— = 1 ğ‘ âˆ£ ğ›½ ğ‘— âˆ£ + ğœ† 2 âˆ‘ ğ‘— = 1 ğ‘ ğ›½ ğ‘— 2 Loss=RSS+Î» 1â€‹\n",
        "\n",
        "j=1 âˆ‘ pâ€‹âˆ£Î² jâ€‹âˆ£+Î» 2â€‹\n",
        "\n",
        "j=1 âˆ‘ pâ€‹Î² j 2â€‹\n",
        "\n",
        "Often expressed as:\n",
        "\n",
        "Loss = RSS + ğœ† [ ğ›¼ âˆ‘ âˆ£ ğ›½ ğ‘— âˆ£ + ( 1 âˆ’ ğ›¼ ) âˆ‘ ğ›½ ğ‘— 2 ] Loss=RSS+Î»[Î±âˆ‘âˆ£Î² jâ€‹âˆ£+(1âˆ’Î±)âˆ‘Î² j 2â€‹] where ğ›¼ âˆˆ [ 0 , 1 ] Î±âˆˆ[0,1] balances L1 and L2.\n",
        "\n",
        "Effect: Combines the sparsity of Lasso with the stability of Ridge.\n",
        "\n",
        "Use Case: Best when there are correlated features or when the number of predictors exceeds the number of observations.\n",
        "\n",
        "Handles multicollinearity.\n",
        "\n",
        "Can select groups of correlated variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "gqp6TSUN6vKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "-Elastic Net is a regularization technique that combines both Lasso (L1) and Ridge (L2) penalties. You should consider using Elastic Net instead of Lasso or Ridge alone in the following situations:\n",
        "\n",
        " When to Use Elastic Net High-dimensional data (p > n)\n",
        "\n",
        "If the number of features p is greater than the number of observations n, Elastic Net often performs better than either Ridge or Lasso alone.\n",
        "\n",
        "Lasso may randomly select one feature from a group of correlated features and ignore the rest, which can be unstable. Elastic Net tends to select groups of correlated variables together.\n",
        "\n",
        "Correlated predictors (multicollinearity)\n",
        "\n",
        "Lasso struggles when predictors are highly correlatedâ€”it tends to arbitrarily select one and drop the others.\n",
        "\n",
        "Ridge shrinks coefficients but does not perform variable selection.\n",
        "\n",
        "Elastic Net does both: it shrinks like Ridge and performs selection like Lasso, but more robustly handles correlated features.\n",
        "\n",
        "You want a compromise between sparsity and shrinkage\n",
        "\n",
        "Lasso gives you sparse solutions (i.e., it sets many coefficients to zero), while Ridge gives small, non-zero coefficients.\n",
        "\n",
        "Elastic Net allows you to tune the balance between the two with the mixing parameter Î±:\n",
        "\n",
        "Î± = 1: Lasso\n",
        "\n",
        "Î± = 0: Ridge\n",
        "\n",
        "0 < Î± < 1: Elastic Net (compromise)"
      ],
      "metadata": {
        "id": "o1EenY636_wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the impact of the regularization parameter (Î») in Logistic Regression?\n",
        "\n",
        "-The regularization parameter ğœ† Î» (often denoted as C in scikit-learn, where ğ¶ = 1 ğœ† C= Î» 1â€‹) in Logistic Regression controls the amount of regularization applied to the model. Regularization is a technique used to prevent overfitting by penalizing large coefficients in the model.\n",
        "\n",
        "ğŸ” Impact of ğœ† Î» in Logistic Regression:\n",
        "\n",
        "When ğœ† Î» is Small (close to 0): Weak regularization (or none at all). The model can fit the training data very closely.\n",
        "\n",
        "Risk of overfitting increases.\n",
        "\n",
        "Coefficients can become large in magnitude.\n",
        "\n",
        "When ğœ† Î» is Large: Strong regularization is applied. The model is constrained, and coefficients are pushed towards zero.\n",
        "\n",
        "Helps prevent overfitting by simplifying the model.\n",
        "\n",
        "May lead to underfitting if too strong, reducing model performance on both training and test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uoHAxuO-7REm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1RAQcn_Z7tmp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRofttj663Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        "-Logistic Regression is a widely used classification algorithm that models the probability of a binary outcome. While it is less restrictive than linear regression in some ways, it still relies on several important assumptions for valid inference and good performance:\n",
        "\n",
        "Key Assumptions of Logistic Regression Binary or Categorical Dependent Variable\n",
        "\n",
        "The dependent variable should be binary (0 or 1) for binary logistic regression. For multinomial or ordinal outcomes, extensions like multinomial or ordinal logistic regression are used.\n",
        "\n",
        "Linearity of Logits\n",
        "\n",
        "Logistic regression assumes a linear relationship between the logit (log-odds) of the outcome and the independent variables:\n",
        "\n",
        "log â¡ ( ğ‘ 1 âˆ’ ğ‘ ) = ğ›½ 0 + ğ›½ 1 ğ‘‹ 1 + â‹¯ + ğ›½ ğ‘˜ ğ‘‹ ğ‘˜ log( 1âˆ’p pâ€‹)=Î² 0â€‹+Î² 1â€‹X 1â€‹+â‹¯+Î² kâ€‹X kâ€‹\n",
        "\n",
        "Note: This is not a linear relationship between the predictors and the outcome itself, but between predictors and the log-odds of the outcome.\n",
        "\n",
        "Independence of Observations\n",
        "\n",
        "Each observation should be independent of the others. This means no repeated measurements or time-series dependencies unless appropriately modeled.\n",
        "\n",
        "No or Little Multicollinearity\n",
        "\n",
        "Independent variables should not be highly correlated with each other. High multicollinearity can inflate the variance of coefficient estimates.\n",
        "\n",
        "Large Sample Size\n",
        "\n",
        "Logistic regression relies on maximum likelihood estimation (MLE), which performs better with larger sample sizes, especially when the outcome is rare.\n",
        "\n",
        "No Extreme Outliers in Predictors\n",
        "\n",
        "Although logistic regression is less sensitive to outliers in the response, outliers in the independent variables can still affect the model.\n",
        "\n",
        "Predictor Variables Can Be Continuous or Categorical\n",
        "\n",
        "Logistic regression handles both types of predictors, but categorical variables should be properly encoded (e.g., one-hot or dummy encoding)."
      ],
      "metadata": {
        "id": "crFlgyFK7ujE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IlIr3PuH632w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "-There are many alternatives to Logistic Regression for classification tasks, each with its strengths depending on the dataset characteristics (size, linearity, noise, etc.). Here are some widely used alternatives:\n",
        "\n",
        "Support Vector Machines (SVM) Strengths: Works well for both linear and non-linear data (using kernels), effective in high-dimensional spaces.\n",
        "Weaknesses: Computationally intensive for large datasets, harder to interpret.\n",
        "\n",
        "Decision Trees Strengths: Simple to interpret, handles both numerical and categorical data, captures non-linear relationships.\n",
        "Weaknesses: Prone to overfitting without pruning or constraints.\n",
        "\n",
        "Random Forest Strengths: Ensemble of decision trees, reduces overfitting, handles missing values and unbalanced data well.\n",
        "Weaknesses: Less interpretable than a single decision tree.\n",
        "\n",
        "Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost) Strengths: High accuracy, handles complex patterns and non-linearity, often the go-to for structured data.\n",
        "Weaknesses: Requires careful tuning, slower training time.\n",
        "\n",
        "k-Nearest Neighbors (k-NN) Strengths: Simple and intuitive, no training phase.\n",
        "Weaknesses: Computationally expensive at prediction time, sensitive to irrelevant features and scaling.\n",
        "\n",
        "Naive Bayes Strengths: Fast, works well with text data and high-dimensional features.\n",
        "Weaknesses: Assumes feature independence, which often doesn't hold.\n",
        "\n",
        "Neural Networks Strengths: Highly flexible, can model very complex relationships.\n",
        "Weaknesses: Requires more data, prone to overfitting, less interpretable, computationally intensive.\n",
        "\n",
        "Linear Discriminant Analysis (LDA) Strengths: Good for linearly separable data, interpretable, works well with small sample sizes.\n",
        "Weaknesses: Assumes normality and equal covariance among classes.\n",
        "\n",
        "Quadratic Discriminant Analysis (QDA) Similar to LDA but allows for class-specific covariance matrices, making it more flexible but also more data-hungry.\n",
        "\n",
        "Rule-based Classifiers (e.g., RIPPER, PART) Strengths: Easy to interpret and explain.\n",
        "\n",
        "Weaknesses: Less powerful for complex patterns."
      ],
      "metadata": {
        "id": "T0TDwnMA72y4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBvebYnv76nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What are Classification Evaluation Metrics?\n",
        "\n",
        "-Classification evaluation metrics are methods used to measure how well a classification model performs. They help you understand how accurately the model predicts the correct class labels. Since classification problems involve assigning inputs into categories, these metrics evaluate the quality of those predictions compared to the actual labels.\n",
        "\n",
        "Here are some of the key classification evaluation metrics:\n",
        "\n",
        "Accuracy Definition: The ratio of correctly predicted instances to the total instances. Formula:\n",
        "\n",
        "Accuracy = Number of correct predictions Total number of predictions Accuracy= Total number of predictions Number of correct predictionsâ€‹\n",
        "\n",
        "When to use: Good when classes are balanced.\n",
        "\n",
        "Precision Definition: The ratio of correctly predicted positive observations to the total predicted positives. Formula:\n",
        "\n",
        "Precision = ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ Precision= TP+FP TPâ€‹\n",
        "\n",
        "Meaning: How many selected items are relevant.\n",
        "\n",
        "Use case: Important when the cost of false positives is high.\n",
        "\n",
        "Recall (Sensitivity or True Positive Rate) Definition: The ratio of correctly predicted positive observations to all actual positives. Formula:\n",
        "\n",
        "Recall = ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ Recall= TP+FN TPâ€‹\n",
        "\n",
        "Meaning: How many relevant items are selected.\n",
        "\n",
        "Use case: Important when the cost of false negatives is high.\n",
        "\n",
        "F1 Score Definition: The harmonic mean of Precision and Recall. Formula:\n",
        "\n",
        "ğ¹ 1 = 2 Ã— Precision Ã— Recall Precision + Recall F1=2Ã— Precision+Recall PrecisionÃ—Recallâ€‹\n",
        "\n",
        "Meaning: Balance between Precision and Recall.\n",
        "\n",
        "Use case: Useful when you want to balance false positives and false negatives.\n",
        "\n",
        "Confusion Matrix A table showing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Predicted Positive Predicted Negative Actual Positive TP FN Actual Negative FP TN\n",
        "\n",
        "Specificity (True Negative Rate) Definition: The ratio of correctly predicted negatives to all actual negatives. Formula:\n",
        "\n",
        "Specificity = ğ‘‡ ğ‘ ğ‘‡ ğ‘ + ğ¹ ğ‘ƒ Specificity= TN+FP TNâ€‹\n",
        "\n",
        "ROC Curve and AUC (Area Under the Curve) ROC Curve: Plots True Positive Rate (Recall) vs. False Positive Rate. AUC: Measures the entire two-dimensional area underneath the ROC curve. A higher AUC indicates better performance."
      ],
      "metadata": {
        "id": "84PncxOv76-j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQYUcRCD8LPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How does class imbalance affect Logistic Regression?\n",
        "\n",
        "-Class imbalance can significantly affect the performance of Logistic Regression and other classification models. Here's how:\n",
        "\n",
        "ğŸ” What is Class Imbalance? Class imbalance occurs when the number of instances in one class far exceeds those in another. For example, in a binary classification problem:\n",
        "\n",
        "Class 0: 95%\n",
        "\n",
        "Class 1: 5%"
      ],
      "metadata": {
        "id": "LS61JB8g8Lyc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "voG_oQn78gQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "-Hyperparameter tuning in logistic regression refers to the process of selecting the best set of hyperparameters that optimize the model's performance.\n",
        "\n",
        "ğŸ” What are Hyperparameters? Hyperparameters are settings that are not learned from the data but are set before training the model. In logistic regression, common hyperparameters include:\n",
        "\n",
        "Regularization type (penalty)\n",
        "\n",
        "Options: 'l1', 'l2', 'elasticnet', 'none'\n",
        "\n",
        "Purpose: Prevent overfitting by adding a penalty for large coefficients.\n",
        "\n",
        "Regularization strength (C)\n",
        "\n",
        "Inverse of regularization strength.\n",
        "\n",
        "Lower C â†’ stronger regularization.\n",
        "\n",
        "Default: C=1.0\n",
        "\n",
        "Solver\n",
        "\n",
        "Algorithms used to optimize the cost function.\n",
        "\n",
        "Options: 'liblinear', 'saga', 'lbfgs', 'newton-cg'\n",
        "\n",
        "Some solvers support specific penalties only.\n",
        "\n",
        "Maximum number of iterations (max_iter)\n",
        "\n",
        "Sets how long the solver should keep optimizing."
      ],
      "metadata": {
        "id": "RVpSkiCR8hBG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zlt_K2s8pCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "-In logistic regression, solvers are algorithms used to optimize the cost function â€” that is, to find the best model parameters (weights). Different solvers have different strengths depending on the dataset size, sparsity, and presence of regularization. Which Solver Should You Use? General Guidelines: Small datasets:\n",
        "\n",
        "Use liblinear â€” fast and reliable for small-scale problems.\n",
        "\n",
        "Large datasets:\n",
        "\n",
        "Use lbfgs, sag, or saga.\n",
        "\n",
        "sag and saga are optimized for large datasets with many samples.\n",
        "\n",
        "L1 regularization (sparse models):\n",
        "\n",
        "Use liblinear or saga.\n",
        "\n",
        "Multinomial classification (not just OvR):\n",
        "\n",
        "Use lbfgs, newton-cg, or saga with multi_class='multinomial'.\n",
        "\n",
        "Sparse data (like text data with TF-IDF):\n",
        "\n",
        "Prefer saga or sag.\n",
        "\n",
        "Recommendation: If unsure and you're not using L1 regularization: Start with lbfgs, as it's robust and works well in most situations.\n",
        "\n",
        "If you need L1 regularization, prefer saga for larger datasets, or liblinear for smaller ones."
      ],
      "metadata": {
        "id": "eZCG2LgW8qjb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdPe7Kp488EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "-Logistic regression is inherently a binary classifier, but it can be extended to handle multiclass classification through several strategies. The most common extensions are:\n",
        "\n",
        "One-vs-Rest (OvR) / One-vs-All (OvA) Concept: Train one binary classifier per class.\n",
        "Each classifier predicts whether a sample belongs to its class or not (i.e., \"class k vs. all others\").\n",
        "\n",
        "For K classes, K separate logistic regression models are trained.\n",
        "\n",
        "At prediction time:\n",
        "\n",
        "Each model outputs a probability.\n",
        "\n",
        "The class with the highest probability is selected.\n",
        "\n"
      ],
      "metadata": {
        "id": "WiTNoZdd9AAu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuCppUaa9Em5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "-Logistic Regression is a widely used statistical method for binary classification problems (predicting one of two possible outcomes). Here is a breakdown of its advantages and disadvantages:\n",
        "\n",
        " Advantages of Logistic Regression Simplicity and Interpretability\n",
        "\n",
        "Easy to implement and interpret.\n",
        "\n",
        "Coefficients provide insight into the relationship between independent variables and the outcome.\n",
        "\n",
        "Computational Efficiency\n",
        "\n",
        "Requires less computational resources compared to more complex models like Random Forests or Neural Networks.\n",
        "\n",
        "Probabilistic Output\n",
        "\n",
        "Produces probabilities for class membership, not just classifications, which is useful for ranking or thresholding.\n",
        "\n",
        "Good Baseline\n",
        "\n",
        "Often used as a baseline model because of its simplicity and robustness.\n",
        "\n",
        "Works Well with Linearly Separable Data\n",
        "\n",
        "Performs well when the data is linearly separable or nearly so.\n",
        "\n",
        "No Need for Feature Scaling (in many cases)\n",
        "\n",
        "Unlike models like SVM or KNN, it doesnâ€™t always require feature scaling (although scaling can still improve performance).\n",
        "\n",
        "Regularization\n",
        "\n",
        "Can be easily extended with L1 (Lasso) or L2 (Ridge) regularization to avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "cxxs62Y29FCy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k58iFBRH9Te_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are some use cases of Logistic Regression?\n",
        "\n",
        "-Logistic Regression is a widely used statistical method for binary classification problems, but it also extends to multi-class classification. Here are some key use cases across different domains:\n",
        "\n",
        "ğŸ” 1. Binary Classification Tasks These are the most common applications of logistic regression:\n",
        "\n",
        "Spam Detection Predict whether an email is spam (1) or not (0).\n",
        "\n",
        "Disease Diagnosis Predict if a patient has a disease (e.g., diabetes, cancer) based on medical indicators.\n",
        "\n",
        "Customer Churn Prediction Determine whether a customer will leave a service or stay.\n",
        "\n",
        "Loan Default Prediction Predict if a borrower will default on a loan.\n",
        "\n",
        "Click-Through Rate Prediction Estimate the probability of a user clicking on an advertisement."
      ],
      "metadata": {
        "id": "YIw5AcI29Tze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "FnwJ49jz9gHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "-Softmax Regression and Logistic Regression are both classification algorithms, but they are used in different scenarios and have key conceptual differences:\n",
        "\n",
        " Logistic Regression Type: Binary classification\n",
        "\n",
        "Use case: When there are only two classes (e.g., spam vs. not spam).\n",
        "\n",
        "Output: A single probability that the input belongs to the \"positive\" class (usually class 1). The probability of the other class (class 0) is simply 1 âˆ’ ğ‘ 1âˆ’p.\n",
        "\n",
        "Activation function: Sigmoid\n",
        "\n",
        "ğœ ( ğ‘§ ) = 1 1 + ğ‘’ âˆ’ ğ‘§ Ïƒ(z)= 1+e âˆ’z\n",
        "\n",
        "1â€‹\n",
        "\n",
        "Decision rule: Classify as 1 if ğœ ( ğ‘§ )\n",
        "\n",
        "0.5 Ïƒ(z)>0.5, else 0.\n",
        "\n",
        " Softmax Regression (also called Multinomial Logistic Regression) Type: Multiclass classification\n",
        "\n",
        "Use case: When there are three or more classes (e.g., classifying types of animals: cat, dog, bird).\n",
        "\n",
        "Output: A vector of probabilities, one for each class. All probabilities sum to1."
      ],
      "metadata": {
        "id": "nIglBbHY9ghI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9p-HV6R9ybs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "-Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors, including the nature of your data, model complexity, training time, and interpretability. Here's a breakdown to help you decide:\n",
        "\n",
        "When to Choose OvR  Simple or Imbalanced Datasets: OvR can be more stable when classes are imbalanced.\n",
        " Modular Design: If you want independent models per class (e.g., easier to debug or deploy separately).\n",
        "\n",
        " Binary-focused Algorithms: When using models that only support binary classification natively (e.g., SVMs, Logistic Regression without softmax).\n",
        "\n",
        " Pros: Easier to implement with binary classifiers.\n",
        "\n",
        "More flexible if you want to focus on a subset of classes.\n",
        "\n",
        " Cons: Doesnâ€™t model inter-class relationships directly.\n",
        "\n",
        "Can be inconsistent if multiple classifiers give high scores.\n",
        "\n",
        " 3. When to Choose Softmax Mutually Exclusive Classes: When exactly one class should be predicted (e.g., classification tasks like digit recognition).\n",
        "\n",
        " Probabilistic Interpretation: Softmax gives proper probability distribution over all classes.\n",
        "\n",
        "Deep Learning Models: Standard choice in neural networks (e.g., image classification with CNNs).\n",
        "\n",
        " Pros: Jointly models all classes â†’ often better accuracy.\n",
        "\n",
        "Single cohesive model.\n",
        "\n",
        "Probabilities sum to 1 â€” intuitive interpretation.\n",
        "\n",
        " Cons: Might struggle with class imbalance unless handled properly.\n",
        "\n",
        "Less interpretable if you want per-class model insights.\n",
        "\n",
        "4. Performance and Scalability OvR: Scales linearly with the number of classes (more models to train).\n",
        "\n",
        "Softmax: Single model, but can be more computationally intensive per iteration."
      ],
      "metadata": {
        "id": "y_YKcP9A9y2u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2sVSNDh-OAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "-In Logistic Regression, the coefficients represent the log-odds of the outcome variable, not direct changes in probabilities. Here's how to interpret them:\n",
        "\n",
        " 1. Log-Odds Interpretation Each coefficient ğ›½ ğ‘— Î² jâ€‹represents the change in the log-odds of the outcome for a one-unit increase in the predictor ğ‘¥ ğ‘— x jâ€‹, holding all other variables constant.\n",
        "\n",
        "logit ( ğ‘ƒ ) = log â¡ ( ğ‘ƒ 1 âˆ’ ğ‘ƒ ) = ğ›½ 0 + ğ›½ 1 ğ‘¥ 1 + â‹¯ + ğ›½ ğ‘— ğ‘¥ ğ‘— logit(P)=log( 1âˆ’P Pâ€‹)=Î² 0â€‹+Î² 1â€‹x 1â€‹+â‹¯+Î² jâ€‹x jâ€‹\n",
        "\n",
        "So, if ğ›½ ğ‘— = 0.5 Î² jâ€‹=0.5, it means that a 1-unit increase in ğ‘¥ ğ‘— x jâ€‹increases the log-odds of the outcome by 0.5.\n",
        "\n",
        " 2. Odds Ratio Interpretation Exponentiate the coefficient to interpret it in terms of odds ratios:\n",
        "\n",
        "Odds Ratio = ğ‘’ ğ›½ ğ‘— Odds Ratio=e Î² jâ€‹\n",
        "\n",
        "If ğ›½ ğ‘— = 0.5 Î² jâ€‹=0.5, then ğ‘’ 0.5 â‰ˆ 1.65 e 0.5 â‰ˆ1.65: the odds of the event happening are 1.65 times higher for a one-unit increase in ğ‘¥ ğ‘— x jâ€‹.\n",
        "\n",
        "If ğ›½ ğ‘— = âˆ’ 0.7 Î² jâ€‹=âˆ’0.7, then ğ‘’ âˆ’ 0.7 â‰ˆ 0.50 e âˆ’0.7 â‰ˆ0.50: the odds are halved for each unit increase in ğ‘¥ ğ‘— x jâ€‹.\n",
        "\n",
        " 3. Probability Interpretation (More Complex) The effect on probability depends on the values of all predictors, since the logistic function is nonlinear:\n",
        "\n",
        "ğ‘ƒ = 1 1 + ğ‘’ âˆ’ ( ğ›½ 0 + ğ›½ 1 ğ‘¥ 1 + â‹¯ + ğ›½ ğ‘— ğ‘¥ ğ‘— ) P= 1+e âˆ’(Î² 0â€‹+Î² 1â€‹x 1â€‹+â‹¯+Î² jâ€‹x jâ€‹)\n",
        "\n",
        "1â€‹\n",
        "\n",
        "So, while the log-odds and odds ratio interpretations are fixed per unit change, the actual change in probability varies depending on where you are on the curve."
      ],
      "metadata": {
        "id": "8uUrOSvp-OYO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNjGskoO-bg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pratical Answer"
      ],
      "metadata": {
        "id": "gV-xGV_9-cd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REEEZpLs-hUR",
        "outputId": "e45dcaef-d030-46c7-9653-caf5f101c5c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')and print the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(C=1, penalty='l1', solver = 'liblinear')\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of the lasso model is {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0y8sg2C_Gz6",
        "outputId": "54cb8924-5c1b-4142-d35a-3232acb343d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the lasso model is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(C=1, penalty='l2', solver = 'lbfgs')\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of the Ridge model is {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"The coefficients of the model is {classifier.coef_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coTLeEru_TYK",
        "outputId": "23c45da2-d20c-4c13-eef0-c3dbe50749c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the Ridge model is 1.0\n",
            "The coefficients of the model is [[ 0.46100411 -0.78836575  2.18624929  0.92865666]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(C=1, penalty='elasticnet', solver = 'saga', l1_ratio=1)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dclruqgF_grR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr\".\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(multi_class='ovr', solver = 'lbfgs')\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F8jAucEP_m0v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x,y =  make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Hyperparameter tuning by using Grid Search CV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\"penalty\":(\"l1\",\"l2\",\"elasticnet\"), \"C\":(1,2,10,15,20,25,30)}\n",
        "\n",
        "#Calling Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Model Training with Grid Search CV\n",
        "clf = GridSearchCV(classifier, param_grid=params, cv=5, verbose=2)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print(\"The best Parameters are \", clf.best_params_)\n",
        "print(\"The accuracy is \", clf.best_score_)\n",
        "\n",
        "\n",
        "\n",
        "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
        "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
        "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l2; total time=   0.2s\n",
        "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
        "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
        "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
        "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=15, penalty=l2; total time=   0.0s\n",
        "[CV] END ...........................C=15, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=15, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=15, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=15, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=15, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=20, penalty=l2; total time=   0.1s\n",
        "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=25, penalty=l2; total time=   0.1s\n",
        "[CV] END ...................................C=25, penalty=l2; total time=   0.0s\n",
        "[CV] END ...........................C=25, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=25, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=25, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=25, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=25, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
        "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
        "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
        "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
        "The best Parameters are  {'C': 1, 'penalty': 'l2'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "bRLg1OUE_t55",
        "outputId": "b122b8ba-673d-48b2-c845-fc64848b9495"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-12-e80d9d63186b>, line 32)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-e80d9d63186b>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    [CV] END ....................................C=1, penalty=l1; total time=   0.0s\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "x,y =  make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Hyperparameter tuning by using KFold\n",
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(classifier, x_train, y_train, cv = cv, scoring='accuracy')\n",
        "scores\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWh7WH0U_4EA",
        "outputId": "6713d000-de55-4d9e-ef04-ca49cd05e174"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84375, 0.775  , 0.8    , 0.8125 , 0.78125])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Load the data using CSV\n",
        "df = pd.read_csv('User_Data.csv')\n",
        "\n",
        "# Seperate x and y\n",
        "x=df[['Age', 'EstimatedSalary']]\n",
        "y=df['Purchased']\n",
        "\n",
        "# Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy Score of this model is {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "YIrwCps1_9eT",
        "outputId": "38f38faf-4269-463a-bd16-539bd3e20ac8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'User_Data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2feba5a3549a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Load the data using CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'User_Data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Load the data using CSV\n",
        "df = pd.read_csv('User_Data.csv')\n",
        "\n",
        "# Seperate x and y\n",
        "x=df[['Age', 'EstimatedSalary']]\n",
        "y=df['Purchased']\n",
        "\n",
        "# Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1)\n",
        "\n",
        "#Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver = 'saga')\n",
        "\n",
        "# Randomised Search CV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params = {\"penalty\":(\"l1\",\"l2\",\"elasticnet\"), 'C' : (1,2,3,10,20,30)}\n",
        "clf = RandomizedSearchCV(lr, param_distributions=params, cv = 5, n_iter=10)\n",
        "\n",
        "# Model Training\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print(f\"The Best Score of this model is {clf.best_score_}\")\n",
        "print(f\"The Best Parameter of this model is {clf.best_params_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Bp5nfhdQAC6W",
        "outputId": "23ea5c63-17d8-4f10-8c34-586a70b0ac06"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'User_Data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d71d91fe98ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Load the data using CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'User_Data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "# Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "clf = OneVsOneClassifier(classifier)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy score is {accuracy_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LalE4hjyALHD",
        "outputId": "cfd6eae5-07a5-4a8a-aeaf-d475be3fc8dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy score is 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperate x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#Evaluation using confusion metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcatH6pSAQNg",
        "outputId": "daa84f8c-9bf2-472c-dd54-99033c656e79"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8,  0],\n",
              "       [ 0, 12]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load the Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperate x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(f\"The Precision score of this model is {precision_score(y_test, y_pred)}\")\n",
        "print(f\"The Recall score of this model is {recall_score(y_test, y_pred)}\")\n",
        "print(f\"The F1 score of this model is {f1_score(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr8zvW07AUmn",
        "outputId": "0c9024c3-4f47-4186-f502-e00bf96ecc63"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Precision score of this model is 1.0\n",
            "The Recall score of this model is 1.0\n",
            "The F1 score of this model is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Load the data using CSV\n",
        "df = pd.read_csv('User_Data.csv')\n",
        "\n",
        "# Seperate x and y\n",
        "x=df[['Age', 'EstimatedSalary']]\n",
        "y=df['Purchased']\n",
        "\n",
        "# Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(class_weight='balanced')\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# Model Prediction\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy Score of this model is {accuracy_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "t4etIXMtAgc8",
        "outputId": "9a29c851-faa8-4e8e-ab53-00114a4873cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'User_Data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7379bc83a63d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Load the data using CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'User_Data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Load the titanic dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis = 1, inplace = True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MYBH2DPvAx4g",
        "outputId": "6056533b-197e-4b3d-c50d-3e5180bdbf14"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e32ad17a9a7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Load the titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ticket'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cabin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PassengerId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.boxplot(data = df['Age'], palette='rocket')\n",
        "plt.title('Variation of Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "0dpc3_45BBKp",
        "outputId": "842e6e99-fa13-42fb-9358-287dcd5a0649"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Age'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Age'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-19cadd717e6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rocket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variation of Age'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Age'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(f\"The Precision score of this model before scaling is {precision_score(y_test, y_pred)}\")\n",
        "print(f\"The Recall score of this model before scaling is {recall_score(y_test, y_pred)}\")\n",
        "print(f\"The F1 score of this model before scaling is {f1_score(y_test, y_pred)}\")\n",
        "\n",
        "# Scaling(Standardization)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x = ss.fit_transform(x)\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(f\"The Precision score of this model after scaling is {precision_score(y_test, y_pred)}\")\n",
        "print(f\"The Recall score of this model after scaling is {recall_score(y_test, y_pred)}\")\n",
        "print(f\"The F1 score of this model after scaling is {f1_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "I_Y4y8MkBfmg",
        "outputId": "619335bd-cae7-4129-a718-5aab62bbedc6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Survived'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ba302bc4d233>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Survived'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "# evaluation using roc-auc curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "y_pred_proba = lr.predict_proba(x_test)[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr)\n",
        "plt.title(\"ROC-AUC Curve\")\n",
        "plt.show()\n",
        "\n",
        "print(\"The performance score is\",auc(fpr,tpr))\n",
        "\n",
        "\n",
        "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
        "\n",
        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "  n_iter_i = _check_optimize_result(\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "76GEEAuvBhRN",
        "outputId": "7e156f89-f420-4e63-b381-571f504cc855"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    n_iter_i = _check_optimize_result(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(C=0.5)\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print(f\"The Precision score of this model is {precision_score(y_test, y_pred)}\")\n",
        "print(f\"The Recall score of this model is {recall_score(y_test, y_pred)}\")\n",
        "print(f\"The F1 score of this model is {f1_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5sDrGcyTBnQ0",
        "outputId": "63abe94f-5cf6-4c43-9cda-9cf20c4acc9e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Survived'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-46408c7c702b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Survived'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "coef = lr.coef_\n",
        "feature_names = x.columns\n",
        "\n",
        "imp_features = pd.DataFrame({'Features':feature_names, 'Coef':coef})\n",
        "imp_features['Coef'] = imp_features['Coef'].abs()\n",
        "imp_features.sort_values(by='Coef', ascending=False)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "nkHEb6r2Br9A",
        "outputId": "3b7b1ef8-ad39-42d8-cc9f-4e9ad2de7b6e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Features      Coef\n",
              "3   petal width (cm)  0.332963\n",
              "2  petal length (cm)  0.191963\n",
              "1   sepal width (cm)  0.147812\n",
              "0  sepal length (cm)  0.033354"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1dcf36f-5460-4692-925c-30894453d007\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>petal width (cm)</td>\n",
              "      <td>0.332963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>petal length (cm)</td>\n",
              "      <td>0.191963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sepal width (cm)</td>\n",
              "      <td>0.147812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sepal length (cm)</td>\n",
              "      <td>0.033354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1dcf36f-5460-4692-925c-30894453d007')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1dcf36f-5460-4692-925c-30894453d007 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1dcf36f-5460-4692-925c-30894453d007');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-acb500d7-3e48-4a1f-ab20-41e1862f08ff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-acb500d7-3e48-4a1f-ab20-41e1862f08ff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-acb500d7-3e48-4a1f-ab20-41e1862f08ff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"imp_features\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Features\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"petal length (cm)\",\n          \"sepal length (cm)\",\n          \"petal width (cm)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coef\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12387295441451752,\n        \"min\": 0.03335401332443643,\n        \"max\": 0.3329629474222884,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.1919630485558248,\n          0.03335401332443643,\n          0.3329629474222884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa Score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=np.array(df['target'])\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#Evaluation\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(f\"The Cohenâ€™s Kappa Score: {cohen_kappa_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "towU_DS9B0Nk",
        "outputId": "db9a2006-dd8b-486f-b24e-ce816b23c187"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Cohenâ€™s Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(C=0.5)\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "precisions = []\n",
        "recalls = []\n",
        "accuracies = []\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "    precision = precision_score(y_test, y_pred_threshold)\n",
        "    recall = recall_score(y_test, y_pred_threshold)\n",
        "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot precision, recall, and accuracy against threshold probabilities\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, precisions, label='Precision')\n",
        "plt.plot(thresholds, recalls, label='Recall')\n",
        "plt.plot(thresholds, accuracies, label='Accuracy')\n",
        "plt.xlabel('Threshold Probability')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and Accuracy vs. Threshold Probability')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
        "\n",
        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "  n_iter_i = _check_optimize_result(\n",
        "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
        "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
        "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VtSiJpezB5QO",
        "outputId": "8be9febd-c47f-4145-ead3-41836f7a7e0d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 50)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    n_iter_i = _check_optimize_result(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, Ibfgs) and compare their accuracy.\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver = 'lbfgs')\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of solver = lbfgs is {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver = 'liblinear')\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of solver = liblinear is {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver = 'saga')\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The accuracy of solver = saga is {accuracy_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qGFvwQLxCB6l",
        "outputId": "564af1b8-c9c2-451c-83f6-19a82fc72353"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Survived'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-06cdc11c32d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, Ibfgs) and compare their accuracy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Survived'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=np.array(df['target'])\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#Evaluation\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(f\"The matthews_corrcoef Score: {matthews_corrcoef(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ym4L40mCF20",
        "outputId": "50d39092-4954-446e-db1c-27024bec11c8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The matthews_corrcoef Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "# Seperate x and y\n",
        "x=df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy score of this model with raw data is {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "# Scaling(Standardization)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x = ss.fit_transform(x)\n",
        "\n",
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The Accuracy score of this model Standardized data is {accuracy_score(y_test, y_pred)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7eV_xw8ECLE6",
        "outputId": "050b6d7f-d047-4052-d844-d1815bbfa33f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Survived'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-58a1daf3b8a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Seperate x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Survived'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "# Seperating x and y\n",
        "x=df.drop('target', axis = 1)\n",
        "y=df['target']\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# Saving the model\n",
        "import joblib\n",
        "joblib.dump(lr, \"Logistic_model.pkl\")\n",
        "print(\"Saved\")\n",
        "\n",
        "# Load the model and predict\n",
        "load_lr = joblib.load(\"Logistic_model.pkl\")\n",
        "print(\"Loaded\")\n",
        "ypred = load_lr.predict(x_test)\n",
        "\n",
        "print(ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duBmwL6LCjWQ",
        "outputId": "cec7d547-f366-47de-e392-3f3fdcba86de"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved\n",
            "Loaded\n",
            "[ 0.87804609  1.06025354 -0.14839367  0.82555349  0.78022649  0.00171271\n",
            " -0.0641215   0.87618886  0.87912578  0.94961952  0.91016637  0.00239393\n",
            "  1.0575802   1.07222653  0.99733711 -0.13770624  0.05898031  0.08664689\n",
            "  1.05502235 -0.05195591]\n"
          ]
        }
      ]
    }
  ]
}